{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "7_OLupUPC2U3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author      : Yi-Chieh Wu, Sriram Sankararman\n",
    "Description : Twitter\n",
    "\"\"\"\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# !!! MAKE SURE TO USE LinearSVC.decision_function(X), NOT LinearSVC.predict(X) !!!\n",
    "# (this makes ''continuous-valued'' predictions)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#I used this to supress warnings from LinearSVC regarding some future updates in the library.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='sklearn.svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47L2XVzBX6c5"
   },
   "source": [
    "# Problem 3: Twitter Analysis Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "9Z8E5YL0CzWe"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "\n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "\n",
    "    np.savetxt(outfile, vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "i67aTAmrGGHi"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "\n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "\n",
    "    word_list = {}\n",
    "    idx = 0\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate word_list\n",
    "        for input_string in fid:\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                if word not in word_list:\n",
    "                    word_list[word] = idx\n",
    "                    idx += 1\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = sum(1 for line in open(infile,'r'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate feature_matrix\n",
    "        for i, input_string in enumerate(fid):\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                feature_matrix[i, word_list[word]] = 1.0\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "-MvTxQPRGOOf"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the\n",
    "    true labels and the predicted labels.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1a: compute classifier performance\n",
    "    if metric == \"accuracy\":\n",
    "        return metrics.accuracy_score(y_true, y_label)\n",
    "    elif metric == \"f1-score\":\n",
    "        return metrics.f1_score(y_true, y_label)\n",
    "    elif metric == \"auroc\":\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    elif metric == \"precision\":\n",
    "        return metrics.precision_score(y_true, y_label)\n",
    "    elif metric == \"sensitivity\":\n",
    "        return metrics.recall_score(y_true, y_label)\n",
    "    elif metric == \"specificity\":\n",
    "        confusion_matrix = metrics.confusion_matrix(y_true, y_label).ravel()\n",
    "        tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "        return (tn)/(tn + fp)\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of LinearSVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: compute average cross-validation performance\n",
    "    accuracy_scores = []\n",
    "    for train_indices, test_indices in kf.split(X,y):\n",
    "        x_train = X[train_indices]\n",
    "        x_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.decision_function(x_test)\n",
    "        accuracy_scores.append(performance(y_test, y_pred, metric))\n",
    "    \n",
    "    return np.mean(accuracy_scores)\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear SVM\n",
    "    \"\"\"\n",
    "\n",
    "    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1c: select optimal hyperparameter using cross-validation\n",
    "    best_C = None\n",
    "    best_accuracy = -(np.inf)\n",
    "    for c in C_range:\n",
    "        clf = LinearSVC(loss='hinge', random_state = 0, C = c)\n",
    "        accuracy_of_clf = cv_performance(clf, X, y, kf, metric)\n",
    "        if accuracy_of_clf > best_accuracy:\n",
    "            best_accuracy = accuracy_of_clf\n",
    "            best_C = c\n",
    "    \n",
    "    return best_C\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of LinearSVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: return performance on test data under a metric.\n",
    "    y_pred = clf.decision_function(X)\n",
    "    return performance(y, y_pred, metric)\n",
    "    ### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "zMIQRGpYErVF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811\n",
      "\n",
      "accuracy :  0.8196428571428571\n",
      "f1-score :  0.8776260952681142\n",
      "auroc :  0.8717871655638666\n",
      "precision :  0.858434814720795\n",
      "sensitivity :  0.9042088607594936\n",
      "specificity :  0.612310606060606\n",
      "The reason why we'd want to maintain class proportion across folds is because doing so ensures that the trends learnt arent biased towards the more prominent data set and also ensures that the model is better suited to generalize to test data which might not be as skewed\n",
      "\n",
      "Linear SVM Hyperparameter Selection based on accuracy:\n",
      "Best C for accuracy is 1.0\n",
      "Linear SVM Hyperparameter Selection based on f1-score:\n",
      "Best C for f1-score is 1.0\n",
      "Linear SVM Hyperparameter Selection based on auroc:\n",
      "Best C for auroc is 1.0\n",
      "Linear SVM Hyperparameter Selection based on precision:\n",
      "Best C for precision is 10.0\n",
      "Linear SVM Hyperparameter Selection based on sensitivity:\n",
      "Best C for sensitivity is 0.001\n",
      "Linear SVM Hyperparameter Selection based on specificity:\n",
      "Best C for specificity is 1.0\n",
      "\n",
      "Performance of best classifier based on accuracy is  0.7428571428571429\n",
      "Performance of best classifier based on f1-score is  0.47058823529411764\n",
      "Performance of best classifier based on auroc is  0.7424684159378038\n",
      "Performance of best classifier based on precision is  0.6363636363636364\n",
      "Performance of best classifier based on sensitivity is  1.0\n",
      "Performance of best classifier based on specificity is  0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# main\n",
    "######################################################################\n",
    "\n",
    "def main() :\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # read the tweets and its labels, change the following two lines to your own path.\n",
    "    ### ========== TODO : START ========== ###\n",
    "    file_path = '../data/tweets.txt'\n",
    "    label_path = '../data/labels.txt'\n",
    "    ### ========== TODO : END ========== ###\n",
    "    dictionary = extract_dictionary(file_path)\n",
    "    print(len(dictionary))\n",
    "    X = extract_feature_vectors(file_path, dictionary)\n",
    "    y = read_vector_file(label_path)\n",
    "    # split data into training (training + cross-validation) and testing set\n",
    "    X_train, X_test = X[:560], X[560:]\n",
    "    y_train, y_test = y[:560], y[560:]\n",
    "\n",
    "    metric_list = [\"accuracy\", \"f1-score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: create stratified folds (5-fold CV)\n",
    "    print()\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    clf = LinearSVC(loss='hinge', random_state=0)\n",
    "    best_C_values = {}\n",
    "    for metric in metric_list:\n",
    "        print(metric,\": \",cv_performance(clf, X_train, y_train, kf, metric))\n",
    "    print(\"The reason why we'd want to maintain class proportion across folds is because doing so ensures that the trends learnt arent biased towards the more prominent data set and also ensures that the model is better suited to generalize to test data which might not be as skewed\")\n",
    "    # part 1c: for each metric, select optimal hyperparameter for linear SVM using CV\n",
    "    #using kf from part 1(b):\n",
    "    print()\n",
    "    for metric in metric_list:\n",
    "        best_C = select_param_linear(X_train, y_train, kf, metric)\n",
    "        best_C_values[metric] = best_C\n",
    "        print(f\"Best C for {metric} is {best_C}\")\n",
    "    # part 2a: train linear SVMs with selected hyperparameters\n",
    "    best_classifiers_per_metric = {}\n",
    "    for metric in metric_list:\n",
    "        clf = LinearSVC(loss='hinge', random_state=0, C=best_C_values[metric])\n",
    "        clf.fit(X_train, y_train)\n",
    "        best_classifiers_per_metric[metric] = clf\n",
    "    # part 2b: test the performance of your classifiers.\n",
    "    print()\n",
    "    for metric in metric_list:\n",
    "        clf = best_classifiers_per_metric[metric]\n",
    "        print(f\"Performance of best classifier based on {metric} is \", performance_test(clf, X_test, y_test, metric))\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W-_mjX0JMes"
   },
   "source": [
    "# Problem 4: Boosting vs. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "0uzCdPTkOQSY"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "DVxef2sxOmVI"
   },
   "outputs": [],
   "source": [
    "class Data :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \"\"\"\n",
    "        Data class.\n",
    "        \n",
    "        Attributes\n",
    "        --------------------\n",
    "            X -- numpy array of shape (n,d), features\n",
    "            y -- numpy array of shape (n,), targets\n",
    "        \"\"\"\n",
    "                \n",
    "        # n = number of examples, d = dimensionality\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.Xnames = None\n",
    "        self.yname = None\n",
    "    \n",
    "    def load(self, filename, header=0, predict_col=-1) :\n",
    "        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n",
    "        \n",
    "        # determine filename\n",
    "        f = filename\n",
    "        \n",
    "        # load data\n",
    "        with open(f, 'r') as fid :\n",
    "            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n",
    "        \n",
    "        # separate features and labels\n",
    "        if predict_col is None :\n",
    "            self.X = data[:,:]\n",
    "            self.y = None\n",
    "        else :\n",
    "            if data.ndim > 1 :\n",
    "                self.X = np.delete(data, predict_col, axis=1)\n",
    "                self.y = data[:,predict_col]\n",
    "            else :\n",
    "                self.X = None\n",
    "                self.y = data[:]\n",
    "        \n",
    "        # load feature and label names\n",
    "        if header != 0:\n",
    "            with open(f, 'r') as fid :\n",
    "                header = fid.readline().rstrip().split(\",\")\n",
    "                \n",
    "            if predict_col is None :\n",
    "                self.Xnames = header[:]\n",
    "                self.yname = None\n",
    "            else :\n",
    "                if len(header) > 1 :\n",
    "                    self.Xnames = np.delete(header, predict_col)\n",
    "                    self.yname = header[predict_col]\n",
    "                else :\n",
    "                    self.Xnames = None\n",
    "                    self.yname = header[0]\n",
    "        else:\n",
    "            self.Xnames = None\n",
    "            self.yname = None\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def load_data(filename, header=0, predict_col=-1) :\n",
    "    \"\"\"Load csv file into Data class.\"\"\"\n",
    "    data = Data()\n",
    "    data.load(filename, header=header, predict_col=predict_col)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "_Zcf4WVqJSpe"
   },
   "outputs": [],
   "source": [
    "# Change the path to your own data directory\n",
    "### ========== TODO : START ========== ###\n",
    "titanic = load_data(\"../data/titanic_train.csv\", header=1, predict_col=0)\n",
    "### ========== TODO : END ========== ###\n",
    "X = titanic.X; Xnames = titanic.Xnames\n",
    "y = titanic.y; yname = titanic.yname\n",
    "n,d = X.shape  # n = number of examples, d =  number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "3Ta7XHRWQGNo"
   },
   "outputs": [],
   "source": [
    "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
    "    \"\"\"\n",
    "    Computes the classifier error over a random split of the data,\n",
    "    averaged over ntrials runs.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf         -- classifier\n",
    "        X           -- numpy array of shape (n,d), features values\n",
    "        y           -- numpy array of shape (n,), target classes\n",
    "        ntrials     -- integer, number of trials\n",
    "        test_size   -- proportion of data used for evaluation\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        train_error -- float, training error\n",
    "        test_error  -- float, test error\n",
    "    \"\"\"\n",
    "\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "\n",
    "    train_scores = []; test_scores = [];\n",
    "    for i in range(ntrials):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split (X,y, test_size = test_size, random_state = i)\n",
    "        clf.fit (xtrain, ytrain)\n",
    "\n",
    "        ypred = clf.predict (xtrain)\n",
    "        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n",
    "        train_scores.append (err)\n",
    "\n",
    "        ypred = clf.predict (xtest)\n",
    "        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n",
    "        test_scores.append (err)\n",
    "\n",
    "    train_error =  np.mean (train_scores)\n",
    "    test_error = np.mean (test_scores)\n",
    "    return train_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "W8-U3un5PjGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Decision Tree...\n",
      "Training error is  0.014044943820224698\n",
      "Testing error is 0.24104895104895108\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(a): Implement the decision tree classifier and report the training error.\n",
    "print('Classifying using Decision Tree...')\n",
    "decision_tree_clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "decision_tree_clf.fit(X,y)\n",
    "y_pred = decision_tree_clf.predict(X)\n",
    "#Used accuracy_score instead of error because question asked us to calculate the training error\n",
    "print(\"Training error is \",(1-metrics.accuracy_score(y, y_pred)))\n",
    "_ , test_error = error(decision_tree_clf, X, y)\n",
    "print(\"Testing error is\", test_error)\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "_x_PevK8Q4dx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Random Forest...\n",
      "The best number of samples is 0.3 with a training error of 0.09427065026362039 and the testing error is 0.1874825174825175\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n",
    "print('Classifying using Random Forest...')\n",
    "max_number_of_samples = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "best_train_error = np.inf\n",
    "best_test_error = np.inf\n",
    "best_no_of_samples = None\n",
    "\n",
    "# Classifying the 'best' classifier based on test_error\n",
    "for maxsamples in max_number_of_samples:\n",
    "    random_forest_classifier = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=maxsamples)\n",
    "    random_forest_classifier.fit(X, y)\n",
    "    training_error, test_error = error(random_forest_classifier, X, y)\n",
    "    if test_error < best_test_error:\n",
    "        best_train_error = training_error\n",
    "        best_test_error = test_error\n",
    "        best_no_of_samples = maxsamples\n",
    "\n",
    "print(f\"The best number of samples is {best_no_of_samples} with a training error of {best_train_error} and the testing error is {best_test_error}\")\n",
    "    \n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "ZFUyPTPwT53v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Random Forest...\n",
      "The best number of features is 3 with a training error of 0.09481546572934976 and the testing error is 0.18678321678321677\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n",
    "print('Classifying using Random Forest...')\n",
    "max_number_of_features = [1,2,3,4,5,6,7]\n",
    "best_max_features = None\n",
    "best_train_error = np.inf\n",
    "best_test_error = np.inf\n",
    "\n",
    "for maxfeatures in max_number_of_features:\n",
    "    random_forest_classifier = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=best_no_of_samples, max_features = maxfeatures)\n",
    "    random_forest_classifier.fit(X,y)\n",
    "    training_error, testing_error = error(random_forest_classifier, X, y)\n",
    "    if testing_error < best_test_error:\n",
    "        best_train_error = training_error\n",
    "        best_test_error = testing_error\n",
    "        best_max_features = maxfeatures\n",
    "    \n",
    "print(f\"The best number of features is {best_max_features} with a training error of {best_train_error} and the testing error is {best_test_error}\")\n",
    "### ========== TODO : END ========== ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
