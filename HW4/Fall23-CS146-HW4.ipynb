{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WLVhlv99E-h"
   },
   "source": [
    "# Problem 1: A Two-Layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jqQZ3TWY9E-i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MRqR8CZ39E-j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (50000, 576)\n",
      "Train target shape:  (50000,)\n",
      "Val data shape:  (10000, 576)\n",
      "Val target shape:  (10000,)\n",
      "Test data shape:  (10000, 576)\n",
      "Test target shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Load fashionMNIST. This is the same code with homework 1.\n",
    "##\n",
    "def crop_center(img,cropped):\n",
    "    img = img.reshape(-1, 28, 28)\n",
    "    start = 28//2-(cropped//2)\n",
    "    img = img[:, start:start+cropped, start:start+cropped]\n",
    "    return img.reshape(-1, cropped*cropped)\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'% kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), 'B', offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(),'B', offset=16).reshape(-1, 784)\n",
    "        images = crop_center(images, 24)\n",
    "    return images, labels\n",
    "X_train_and_val, y_train_and_val = load_mnist('./data/mnist', kind='train')\n",
    "X_test, y_test = load_mnist('./data/mnist', kind='t10k')\n",
    "X_train, X_val = X_train_and_val[:50000], X_train_and_val[50000:]\n",
    "y_train, y_val = y_train_and_val[:50000], y_train_and_val[50000:]\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Val data shape: ', X_val.shape)\n",
    "print('Val target shape: ', y_val.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xjfsY8Ox9E-j"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is Odd\n"
     ]
    }
   ],
   "source": [
    "# PART (a):\n",
    "# To Visualize a point in the dataset\n",
    "index = 10\n",
    "X = np.array(X_train[index], dtype='uint8').reshape([24, 24])\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "if y_train[index] in set([1, 3, 5, 7, 9]):\n",
    "    label = 'Odd'\n",
    "else:\n",
    "    label = 'Even'\n",
    "print('Label is', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ozhJL4T9E-j"
   },
   "source": [
    "In the following cells, you will build a two-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Mo9zmasa9E-j"
   },
   "outputs": [],
   "source": [
    "# convert to binary label\n",
    "y_train = y_train.astype(int) % 2\n",
    "y_val = y_val.astype(int) % 2\n",
    "y_test = y_test.astype(int) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WzU3563s9E-j"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network for binary classification.\n",
    "    We train the network with a softmax output and cross entropy loss function\n",
    "    with L2 regularization on the weight matrices. The network uses a ReLU\n",
    "    nonlinearity after the first fully connected layer.\n",
    "    Input: X\n",
    "    Hidden states for layer 1: h1 = XW1 + b1\n",
    "    Activations: a1 = ReLU(h1)\n",
    "    Hidden states for layer 2: h2 = a1W2 + b2\n",
    "    Probabilities: s = softmax(h2)\n",
    "\n",
    "    ReLU function:\n",
    "    (i) x = x if x >= 0  (ii) x = 0 if x < 0\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the output of the neural network using forward pass.\n",
    "        #   The expected result should be a matrix of shape (N, C), where:\n",
    "        #     - N is the number of examples in the input dataset 'X'.\n",
    "        #     - C is the number of classes.\n",
    "        #   Use 'h1' as the first hidden layer output\n",
    "        #   Apply the ReLU activation function to 'h1' to get 'a1'. Use np.maximum for ReLU implementation.\n",
    "        #   The output 'scores' is the result of the second layer (before applying softmax).\n",
    "        #   Refer to the model architecture comments at the beginning of this class for more details.\n",
    "        #   Note: Do not use a for loop in your implementation.\n",
    "        ##  Part (b): Implement the forward pass and compute scores.\n",
    "        h1 = np.dot(X, W1) + b1\n",
    "        a1 = np.maximum(h1, 0)\n",
    "        scores = np.dot(a1, W2) + b2\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "\n",
    "        # scores is num_examples by num_classes (N, C)\n",
    "        def softmax_loss(x, y):\n",
    "            ### ========== TODO : START ========== ###\n",
    "            #   Calculate the cross entropy loss after softmax output layer.\n",
    "            #   This function should return loss and dx\n",
    "            probs = np.exp(x - np.max(x, axis=1, keepdims=True)) # Other Notes: this operation is called stable softmax: numerically more stable as it reduces overflow issues by not letting the numerator and denominator grow too big.\n",
    "            probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "            N = x.shape[0]\n",
    "            ##  Part (d): Implement the CrossEntropyLoss\n",
    "            loss = None\n",
    "            loss = -np.log(probs[range(N), y])\n",
    "            data_loss = np.sum(loss) / N\n",
    "            ##  Part (d): Implement the gradient of y wrt x\n",
    "            dx = probs\n",
    "            dx[range(N), y] -=1\n",
    "            dx /= N\n",
    "            ### ========== TODO : END ========== ###\n",
    "            return loss, dx\n",
    "\n",
    "\n",
    "        data_loss, dscore = softmax_loss(scores, y)\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the regularization loss. Multiply the regularization\n",
    "        #   loss by 0.5 (in addition to the regularization factor 'reg').\n",
    "        ##  Part (c): Implement the regularization loss\n",
    "        reg_loss = None\n",
    "        reg_loss = 0.5 * reg * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        grads = {}\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #  Compute backpropagation\n",
    "        #  Remember the loss contains two parts: cross-entropy and regularization. The computation for gradients of W1 and b1 shown here can be regarded as a reference.\n",
    "        ## Part (e): Implement the computations of gradients for W2 and b2.\n",
    "        grads['W2'] = np.dot(a1.T, dscore) + reg * W2\n",
    "        grads['b2'] = np.ones(N).dot(dscore)\n",
    "        \n",
    "        dh = np.dot(dscore, W2.T)\n",
    "        dh[a1 <= 0] = 0\n",
    "\n",
    "        grads['W1'] = np.dot(X.T, dh) + reg * W1\n",
    "        grads['b1'] = np.ones(N).dot(dh)\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "          X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in np.arange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #   Create a minibatch (X_batch, y_batch) by sampling batch_size\n",
    "            #   samples randomly.\n",
    "\n",
    "            b_index = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[b_index]\n",
    "            y_batch = y[b_index]\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "\n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['b1'] -= learning_rate * grads['b1']\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b2'] -= learning_rate * grads['b2']\n",
    "\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "          classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "          to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Predict the class given the input data.\n",
    "        ##  Part (f): Implement the prediction function\n",
    "        y_pred = None\n",
    "\n",
    "        # Compute the forward pass\n",
    "        h1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        a1 = np.maximum(h1, 0)\n",
    "        scores = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "\n",
    "        # Predict the class with the highest score for each data point\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "        ### ========== TODO : END ========== ###\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "O1NPs0Jy9E-k",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  1e-05\n",
      "Validation accuracy:  0.6978\n",
      "Test accuracy (subopt_net):  0.6978\n",
      "\n",
      "\n",
      "learning_rate:  0.0001\n",
      "Validation accuracy:  0.8843\n",
      "Test accuracy (subopt_net):  0.8812\n",
      "\n",
      "\n",
      "learning_rate:  0.001\n",
      "Validation accuracy:  0.9698\n",
      "Test accuracy (subopt_net):  0.9702\n",
      "\n",
      "\n",
      "learning_rate:  0.005\n",
      "Validation accuracy:  0.8865\n",
      "Test accuracy (subopt_net):  0.8734\n",
      "\n",
      "\n",
      "learning_rate:  0.1\n",
      "Validation accuracy:  0.506\n",
      "Test accuracy (subopt_net):  0.5074\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 576\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "for learning_rate in [1e-5, 1e-4, 1e-3, 5e-3, 1e-1]:\n",
    "  print('learning_rate: ', learning_rate)\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=200,\n",
    "              learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "              reg=0.1, verbose=False)\n",
    "\n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  # Save this net as the variable subopt_net for later comparison.\n",
    "  subopt_net = net\n",
    "  test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "  print('Test accuracy (subopt_net): ', test_acc)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-RDGlMK9E-k"
   },
   "source": [
    "# Problem 2: K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700340896141,
     "user": {
      "displayName": "SIYAN ZHAO",
      "userId": "02511534054634309772"
     },
     "user_tz": 480
    },
    "id": "HXaEq7BY9E-k"
   },
   "outputs": [],
   "source": [
    "## Function to load the CIFAR10 data\n",
    "## Documentation of CIFAR10: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def dataloader():\n",
    "  import tensorflow as tf\n",
    "  cifar10 = tf.keras.datasets.cifar10\n",
    "  (_, _), (X, y) = cifar10.load_data()\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5Mvwjg2T9E-k"
   },
   "outputs": [],
   "source": [
    "## simple utility function to visualize the data\n",
    "def visualize(X, ind):\n",
    "  from PIL import Image\n",
    "  plt.imshow(Image.fromarray(X[ind], 'RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "PHg7PX6z9E-k"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m dataloader()\n",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m, in \u001b[0;36mdataloader\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataloader\u001b[39m():\n\u001b[1;32m----> 4\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m   cifar10 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mcifar10\n\u001b[0;32m      6\u001b[0m   (_, _), (X, y) \u001b[38;5;241m=\u001b[39m cifar10\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "X, y = dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2X7wZU6n9E-k"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 10K images of size 32 x 32 x 3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# where 32 x 32 is the height and width of the image\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 3 is the number of channels 'RGB'\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# 10K images of size 32 x 32 x 3\n",
    "# where 32 x 32 is the height and width of the image\n",
    "# 3 is the number of channels 'RGB'\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cx-yZmli9E-k"
   },
   "outputs": [],
   "source": [
    "visualize(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPxMMp0H9E-k"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Implement this function to form a 10000 x N matrix\n",
    "  from 10000 x 32 x 32 x 3 shape input.\n",
    "'''\n",
    "def reshape(X):\n",
    "  '''\n",
    "    Write one line of code here\n",
    "  '''\n",
    "  ### ========== TODO : START ========== ###\n",
    "  # part (a)\n",
    "\n",
    "  ### ========== TODO : END ========== ###\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsCg1dGe9E-k"
   },
   "outputs": [],
   "source": [
    "X = reshape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMO2nNTr9E-k"
   },
   "outputs": [],
   "source": [
    "clustering_score = []\n",
    "for i in tqdm(range(5, 20, 5)):\n",
    "  score = 0\n",
    "  for rs in tqdm(range(3)):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'random', random_state = rs)\n",
    "    '''\n",
    "      Write one line of code to fit the kMeans algorithm to the data\n",
    "      Write another line of code to report the kMeans clustering score\n",
    "      defined as sum of squared distances of samples to their closest\n",
    "      cluster center, weighted by the sample weights if provided.\n",
    "      Hint: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    '''\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part (b)\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "  clustering_score.append(score/3) ## divide by 3 because 3 random states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYPDISvk9E-k"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Submit the plot you get after running this piece of code in your solutions\n",
    "'''\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(5, 20, 5), clustering_score)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('Clustering Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXX-UYJk9E-k"
   },
   "source": [
    "### Visualize K Clusters for K = 10 and random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSGKi7vd9E-l"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "#Transform the data\n",
    "df = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DILDrpjR9E-l"
   },
   "outputs": [],
   "source": [
    "### Analyzing the input data in 2D based on its true labels\n",
    "\n",
    "u_labels = np.unique(y[:, 0])\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(df[y[:, 0] == i , 0] , df[y[:, 0] == i , 1] , label = i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD90h31o9E-l"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Submit the output plot as a part of the solutions\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters = 10, init = 'random', random_state = 42)\n",
    "'''\n",
    "  Write 1 - 2 line of code to get the predicted labels of the 10-clusters\n",
    "'''\n",
    "### ========== TODO : START ========== ###\n",
    "\n",
    "### ========== TODO : END ========== ###\n",
    "\n",
    "u_labels = np.unique(label)\n",
    "\n",
    "#plotting the results:\n",
    "\n",
    "for i in u_labels:\n",
    "    '''\n",
    "      Write one line of code to get a scatter plot for i-th cluster.\n",
    "      Have its label = i\n",
    "    '''\n",
    "    ### ========== TODO : START ========== ###\n",
    "\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTqbZsZM9E-l"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
